# excel_merger.py
import streamlit as st
import pandas as pd
import numpy as np
from functools import reduce
from io import BytesIO
import re
from datetime import datetime
import os
from pathlib import Path
import sqlite3
import json

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å AgGrid; graceful fallback –∫ st.dataframe –µ—Å–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
try:
    from st_aggrid import AgGrid, GridOptionsBuilder
    AGGRID_AVAILABLE = True
except Exception:
    AGGRID_AVAILABLE = False

# ---------------------- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—É—Ç–µ–π ----------------------
try:
    BASE_DIR = Path(__file__).parent.resolve()
except NameError:
    BASE_DIR = Path.cwd()

DB_PATH = BASE_DIR / "merged_history.db"
MERGED_DIR = BASE_DIR / "merged_files"
MERGED_DIR.mkdir(parents=True, exist_ok=True)

# ---------------------- –ü–µ—Ä–µ–≤–æ–¥—ã ----------------------
translations = {
    "en": {
        "title": "üìä Merge Multiple Excel Files by ID",
        "upload": "üìÇ Upload Excel files (.xlsx)",
        "id_select": "üîë Select ID column and fields for each file",
        "id_help": "Choose the column to match records (ID / code / number / user, etc.)",
        "include_cols": "‚úÖ Columns to include",
        "filters": "üîé Filters before merge",
        "prefix": "Add filename prefix to all columns (except ID) to avoid conflicts",
        "merge_button": "üöÄ Merge",
        "preview": "üëÄ Preview (unmatched rows at the bottom and in red)",
        "download_clean": "‚¨áÔ∏è Download {name}.xlsx (clean)",
        "download_colored": "‚¨áÔ∏è Download {name}_colored.xlsx (with highlights)",
        "download_filtered_each": "‚¨áÔ∏è Download per-file filtered data",
        "info_upload": "Upload at least **2 Excel files** (.xlsx) to start.",
        "info_wait": "Select ID, choose columns & filters, then click ¬´Merge¬ª.",
        "warn_duplicates": "In file **{name}** found duplicates by selected ID: {count}. Keeping only the first.",
        "error_read": "‚ö†Ô∏è Could not read file {name}: {error}",
        "footer": "¬© All rights reserved. Made by Mashrabjon",
        "error_no_id": "File {name} has no 'id' column after normalization.",
        "error_merge": "Error while merging: {error}",
        "error_styled": "Could not create colored Excel: {error}\nTry updating pandas/openpyxl.",
        "expander": "‚ÑπÔ∏è Details",
        "expander_text": "- **ID** is converted to string and trimmed.\n"
                         "- Merge is **outer/inner/left/right** as chosen.\n"
                         "- `__unmatched = True` means the ID is missing in at least one file.\n"
                         "- In the clean file, helper columns are hidden; unmatched IDs go to the bottom.",
        "join_type": "üîó Join type",
        "nan_fill": "Replace NaN with:",
        "history": "üóÇÔ∏è Merged files history (persistent)",
        "metrics": "üìà Merge summary",
        "m_total": "Total rows",
        "m_matched": "Fully matched",
        "m_unmatched": "Unmatched",
        "m_files_presence": "Presence by number of files",
        "theme": "üé® Theme",
        "light": "üåû Light",
        "dark": "üåô Dark",
        "clear_history": "üóëÔ∏è Clear merged history",
        "save_merged": "üíæ Save merged to server (add to history)",
        "auto_save": "üîÅ Auto-save merged to server"
    },
    "ru": {
        "title": "üìä –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö Excel –ø–æ ID",
        "upload": "üìÇ –ó–∞–≥—Ä—É–∑–∏—Ç–µ Excel —Ñ–∞–π–ª—ã (.xlsx)",
        "id_select": "üîë –í—ã–±–æ—Ä ID-–∫–æ–ª–æ–Ω–∫–∏ –∏ –ø–æ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞",
        "id_help": "–í—ã–±–µ—Ä–∏—Ç–µ –∫–æ–ª–æ–Ω–∫—É, –ø–æ –∫–æ—Ç–æ—Ä–æ–π —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å–∏ (ID/–∫–æ–¥/–Ω–æ–º–µ—Ä/–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∏ —Ç.–ø.)",
        "include_cols": "‚úÖ –ö–æ–ª–æ–Ω–∫–∏ –¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è",
        "filters": "üîé –§–∏–ª—å—Ç—Ä—ã –¥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è",
        "prefix": "–î–æ–±–∞–≤–ª—è—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å —Å –∏–º–µ–Ω–µ–º —Ñ–∞–π–ª–∞ –∫–æ –≤—Å–µ–º –∫–æ–ª–æ–Ω–∫–∞–º (–∫—Ä–æ–º–µ ID), —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π",
        "merge_button": "üöÄ –û–±—ä–µ–¥–∏–Ω–∏—Ç—å",
        "preview": "üëÄ –ü—Ä–µ–¥–ø—Ä–æ—Å–º–æ—Ç—Ä (–Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤–Ω–∏–∑—É –∏ –∫—Ä–∞—Å–Ω—ã–º)",
        "download_clean": "‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å {name}.xlsx (—á–∏—Å—Ç—ã–π)",
        "download_colored": "‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å {name}_colored.xlsx (—Å –ø–æ–¥—Å–≤–µ—Ç–∫–æ–π)",
        "download_filtered_each": "‚¨áÔ∏è –°–∫–∞—á–∞—Ç—å –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–∞–∂–¥–æ–º—É —Ñ–∞–π–ª—É",
        "info_upload": "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –∫–∞–∫ –º–∏–Ω–∏–º—É–º **2 Excel-—Ñ–∞–π–ª–∞** (.xlsx), —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å.",
        "info_wait": "–í—ã–±–µ—Ä–∏—Ç–µ ID, –∫–æ–ª–æ–Ω–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä—ã, –∑–∞—Ç–µ–º –Ω–∞–∂–º–∏—Ç–µ ¬´–û–±—ä–µ–¥–∏–Ω–∏—Ç—å¬ª.",
        "warn_duplicates": "–í —Ñ–∞–π–ª–µ **{name}** –Ω–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ –≤—ã–±—Ä–∞–Ω–Ω–æ–º—É ID: {count}. –û—Å—Ç–∞–≤–ª—è—é –ø–µ—Ä–≤—É—é –∑–∞–ø–∏—Å—å.",
        "error_read": "‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {name}: {error}",
        "footer": "¬© –í—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã. –°–¥–µ–ª–∞–Ω–æ –ú–∞—à—Ä–∞–±–∂–æ–Ω",
        "error_no_id": "–í —Ñ–∞–π–ª–µ {name} –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ 'id' –ø–æ—Å–ª–µ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏.",
        "error_merge": "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏: {error}",
        "error_styled": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ü–≤–µ—Ç–Ω–æ–π Excel: {error}\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å pandas/openpyxl.",
        "expander": "‚ÑπÔ∏è –ü–æ—è—Å–Ω–µ–Ω–∏—è",
        "expander_text": "- **ID** –ø—Ä–∏–≤–æ–¥–∏—Ç—Å—è –∫ —Å—Ç—Ä–æ–∫–µ –∏ –æ—á–∏—â–∞–µ—Ç—Å—è –æ—Ç –ø—Ä–æ–±–µ–ª–æ–≤.\n"
                         "- –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ ‚Äî **outer/inner/left/right** –ø–æ –≤—ã–±–æ—Ä—É.\n"
                         "- `__unmatched = True` –æ–∑–Ω–∞—á–∞–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ ID —Ö–æ—Ç—è –±—ã –≤ –æ–¥–Ω–æ–º —Ñ–∞–π–ª–µ.\n"
                         "- –í ¬´—á–∏—Å—Ç–æ–º¬ª —Ñ–∞–π–ª–µ —Å–ª—É–∂–µ–±–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å–∫—Ä—ã—Ç—ã; –Ω–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ ‚Äî –≤–Ω–∏–∑—É.",
        "join_type": "üîó –¢–∏–ø –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è",
        "nan_fill": "–ó–∞–º–µ–Ω—è—Ç—å NaN –Ω–∞:",
        "history": "üóÇÔ∏è –ò—Å—Ç–æ—Ä–∏—è –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–ø–æ—Å—Ç–æ—è–Ω–Ω–æ)",
        "metrics": "üìà –°–≤–æ–¥–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è",
        "m_total": "–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫",
        "m_matched": "–ü–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–ø–∞–≤—à–∏–µ",
        "m_unmatched": "–ù–µ—Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ",
        "m_files_presence": "–ü—Ä–∏—Å—É—Ç—Å—Ç–≤–∏–µ –ø–æ —á–∏—Å–ª—É —Ñ–∞–π–ª–æ–≤",
        "theme": "üé® –¢–µ–º–∞",
        "light": "üåû –°–≤–µ—Ç–ª–∞—è",
        "dark": "üåô –¢—ë–º–Ω–∞—è",
        "clear_history": "üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–π",
        "save_merged": "üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –Ω–∞ —Å–µ—Ä–≤–µ—Ä (–¥–æ–±–∞–≤–∏—Ç—å –≤ –∏—Å—Ç–æ—Ä–∏—é)",
        "auto_save": "üîÅ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π –Ω–∞ —Å–µ—Ä–≤–µ—Ä"
    },
    "uz": {
        "title": "üìä Bir nechta Excel fayllarini ID bo‚Äòyicha birlashtirish",
        "upload": "üìÇ Excel fayllarini yuklang (.xlsx)",
        "id_select": "üîë Har bir fayl uchun ID ustuni va maydonlarni tanlang",
        "id_help": "Moslashtirish uchun ustunni tanlang (ID / kod / raqam / foydalanuvchi va h.k.)",
        "include_cols": "‚úÖ Kiritiladigan ustunlar",
        "filters": "üîé Birlashtirishdan oldingi filtrlar",
        "prefix": "ID dan tashqari barcha ustunlarga fayl nomi prefiksi qo‚Äòshilsin",
        "merge_button": "üöÄ Birlashtirish",
        "preview": "üëÄ Oldindan ko‚Äòrish (mos kelmagan satrlar pastda va qizil)",
        "download_clean": "‚¨áÔ∏è {name}.xlsx (toza) yuklab olish",
        "download_colored": "‚¨áÔ∏è {name}_colored.xlsx (rangli) yuklab olish",
        "download_filtered_each": "‚¨áÔ∏è Har fayl uchun filtrlangan ma‚Äôlumotni yuklab olish",
        "info_upload": "Boshlash uchun kamida **2 ta Excel fayli** (.xlsx) yuklang.",
        "info_wait": "ID, ustunlar va filtrlarni tanlang, so‚Äòng ¬´Birlashtirish¬ª.",
        "warn_duplicates": "**{name}** faylida {count} ta dublikat ID topildi. Faqat birinchi qoldi.",
        "error_read": "‚ö†Ô∏è {name} faylini o‚Äòqib bo‚Äòlmadi: {error}",
        "footer": "¬© Barcha huquqlar himoyalangan. Mashrabjon tomonidan yaratilgan",
        "error_no_id": "{name} faylida 'id' ustuni yo‚Äòq.",
        "error_merge": "Birlashtirishda xato: {error}",
        "error_styled": "Rangli Excel yaratib bo‚Äòlmadi: {error}\nPandas/openpyxl ni yangilang.",
        "expander": "‚ÑπÔ∏è Izohlar",
        "expander_text": "- **ID** matn ko‚Äòrinishiga o‚Äòtkaziladi va tozalanadi.\n"
                         "- Birlashtirish ‚Äî tanlangan **outer/inner/left/right**.\n"
                         "- `__unmatched = True` ‚Äî ID kamida bitta faylda yo‚Äòq.\n"
                         "- ¬´Toza¬ª faylda xizmat ustunlari yashiriladi; nomutanosiblar pastda.",
        "join_type": "üîó Birlashtirish turi",
        "nan_fill": "NaN o‚Äòrniga:",
        "history": "üóÇÔ∏è Birlashtirilgan fayllar tarixi (doimiy)",
        "metrics": "üìà Birlashtirish xulosasi",
        "m_total": "Jami qatorlar",
        "m_matched": "To‚Äòliq mos kelgan",
        "m_unmatched": "Mos kelmagan",
        "m_files_presence": "Fayllar bo‚Äòyicha mavjudlik",
        "theme": "üé® Mavzu",
        "light": "üåû Yorug‚Äò",
        "dark": "üåô Qorong‚Äòi",
        "clear_history": "üóëÔ∏è Tarixni o‚Äòchirish",
        "save_merged": "üíæ Birlashtirilganni serverga saqlash (tarixga qo'shish)",
        "auto_save": "üîÅ Avto-saqlash"
    },
    "ko": {
        "title": "üìä Ïó¨Îü¨ Excel ÌååÏùºÏùÑ IDÎ°ú Î≥ëÌï©",
        "upload": "üìÇ Excel ÌååÏùº ÏóÖÎ°úÎìú (.xlsx)",
        "id_select": "üîë Í∞Å ÌååÏùºÏùò ID Ïó¥ Î∞è ÌïÑÎìú ÏÑ†ÌÉù",
        "id_help": "Î†àÏΩîÎìúÎ•º ÏùºÏπòÏãúÌÇ¨ Ïó¥ÏùÑ ÏÑ†ÌÉùÌïòÏÑ∏Ïöî (ID / ÏΩîÎìú / Î≤àÌò∏ / ÏÇ¨Ïö©Ïûê Îì±)",
        "include_cols": "‚úÖ Ìè¨Ìï®Ìï† Ïó¥",
        "filters": "üîé Î≥ëÌï© Ï†Ñ ÌïÑÌÑ∞",
        "prefix": "ID Ï†úÏô∏ Î™®Îì† Ïó¥Ïóê ÌååÏùºÎ™Ö Ï†ëÎëêÏÇ¨ Ï∂îÍ∞Ä",
        "merge_button": "üöÄ Î≥ëÌï©",
        "preview": "üëÄ ÎØ∏Î¶¨Î≥¥Í∏∞ (Î∂àÏùºÏπò ÌñâÏùÄ ÏïÑÎûòÏ™Ω Îπ®Í∞ÑÏÉâ)",
        "download_clean": "‚¨áÔ∏è {name}.xlsx (ÌÅ¥Î¶∞)",
        "download_colored": "‚¨áÔ∏è {name}_colored.xlsx (Í∞ïÏ°∞)",
        "download_filtered_each": "‚¨áÔ∏è ÌååÏùºÎ≥Ñ ÌïÑÌÑ∞ÎßÅ Îç∞Ïù¥ÌÑ∞ Îã§Ïö¥Î°úÎìú",
        "info_upload": "ÏãúÏûëÌïòÎ†§Î©¥ ÏµúÏÜå **2Í∞ú**Ïùò .xlsx ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî.",
        "info_wait": "ID, Ïó¥, ÌïÑÌÑ∞Î•º ÏÑ†ÌÉùÌïú ÌõÑ ¬´Î≥ëÌï©¬ª.",
        "warn_duplicates": "**{name}** ÌååÏùºÏóê ÏÑ†ÌÉùÌïú ID Í∏∞Ï§Ä Ï§ëÎ≥µ {count}Í∞ú Î∞úÍ≤¨. Ï≤´ Î≤àÏß∏Îßå Ïú†ÏßÄ.",
        "error_read": "‚ö†Ô∏è {name} ÌååÏùºÏùÑ ÏùΩÏùÑ Ïàò ÏóÜÏäµÎãàÎã§: {error}",
        "footer": "¬© Î™®Îì† Í∂åÎ¶¨ Î≥¥Ïú†. Mashrabjon Ï†úÏûë",
        "error_no_id": "{name} ÌååÏùºÏóê 'id' Ïó¥Ïù¥ ÏóÜÏäµÎãàÎã§.",
        "error_merge": "Î≥ëÌï© Ï§ë Ïò§Î•ò: {error}",
        "error_styled": "ÏÉâÏÉÅ Excel ÏÉùÏÑ± Ïã§Ìå®: {error}\nPandas/openpyxl ÏóÖÎç∞Ïù¥Ìä∏ Í∂åÏû•.",
        "expander": "‚ÑπÔ∏è ÏÑ§Î™Ö",
        "expander_text": "- **ID**Îäî Î¨∏ÏûêÏó¥Î°ú Î≥ÄÌôò ÌõÑ Í≥µÎ∞± Ï†úÍ±∞.\n"
                         "- Î≥ëÌï©ÏùÄ ÏÑ†ÌÉùÌïú **outer/inner/left/right**.\n"
                         "- `__unmatched = True`Îäî Ï†ÅÏñ¥ÎèÑ ÌïòÎÇòÏùò ÌååÏùºÏóê ÏóÜÏùå.\n"
                         "- ÌÅ¥Î¶∞ ÌååÏùºÏùÄ Î≥¥Ï°∞ Ïó¥ Ïà®ÍπÄ; Î∂àÏùºÏπòÎäî ÏïÑÎûò.",
        "join_type": "üîó Ï°∞Ïù∏ Î∞©Ïãù",
        "nan_fill": "NaN ÎåÄÏ≤¥Í∞í:",
        "history": "üóÇÔ∏è Î≥ëÌï©Îêú ÌååÏùº Í∏∞Î°ù (ÏòÅÍµ¨)",
        "metrics": "üìà Î≥ëÌï© ÏöîÏïΩ",
        "m_total": "Ï¥ù ÌñâÏàò",
        "m_matched": "ÏôÑÏ†Ñ ÏùºÏπò",
        "m_unmatched": "Î∂àÏùºÏπò",
        "m_files_presence": "ÌååÏùº ÏàòÎ≥Ñ Ï°¥Ïû¨Ïó¨Î∂Ä",
        "theme": "üé® ÌÖåÎßà",
        "light": "üåû ÎùºÏù¥Ìä∏",
        "dark": "üåô Îã§ÌÅ¨",
        "clear_history": "üóëÔ∏è Î≥ëÌï© Í∏∞Î°ù ÏÇ≠Ï†ú",
        "save_merged": "üíæ Î≥ëÌï© ÌååÏùºÏùÑ ÏÑúÎ≤ÑÏóê Ï†ÄÏû• (Í∏∞Î°ù Ï∂îÍ∞Ä)",
        "auto_save": "üîÅ ÏûêÎèô Ï†ÄÏû•"
    }
}

# ---------------------- DB (SQLite) helpers ----------------------
def init_db():
    """–°–æ–∑–¥–∞—ë—Ç —Ç–∞–±–ª–∏—Ü—É merged_files –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç."""
    conn = sqlite3.connect(str(DB_PATH))
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS merged_files (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            basename TEXT NOT NULL,
            clean_path TEXT NOT NULL,
            colored_path TEXT,
            rows INTEGER,
            cols INTEGER,
            created_at TEXT
        )
    """)
    conn.commit()
    conn.close()

def add_record_db(basename: str, clean_path: str, colored_path: str, rows: int, cols: int):
    conn = sqlite3.connect(str(DB_PATH))
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO merged_files (basename, clean_path, colored_path, rows, cols, created_at)
        VALUES (?, ?, ?, ?, ?, ?)
    """, (basename, clean_path, colored_path, int(rows), int(cols), datetime.now().strftime("%Y-%m-%d %H:%M")))
    conn.commit()
    last_id = cur.lastrowid
    conn.close()
    return last_id

def get_all_records_db():
    conn = sqlite3.connect(str(DB_PATH))
    cur = conn.cursor()
    cur.execute("SELECT id, basename, clean_path, colored_path, rows, cols, created_at FROM merged_files ORDER BY id DESC")
    rows = cur.fetchall()
    conn.close()
    records = []
    for r in rows:
        records.append({
            "id": int(r[0]),
            "basename": r[1],
            "clean_path": r[2],
            "colored_path": r[3],
            "rows": int(r[4]) if r[4] is not None else None,
            "cols": int(r[5]) if r[5] is not None else None,
            "created_at": r[6]
        })
    return records

def delete_record_db(record_id: int, delete_files: bool = True):
    """–£–¥–∞–ª—è–µ—Ç –∑–∞–ø–∏—Å—å –∏–∑ –ë–î; –ø–æ –æ–ø—Ü–∏–∏ —É–¥–∞–ª—è–µ—Ç –∏ —Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ —Ñ–∞–π–ª—ã."""
    conn = sqlite3.connect(str(DB_PATH))
    cur = conn.cursor()
    cur.execute("SELECT clean_path, colored_path FROM merged_files WHERE id=?", (record_id,))
    row = cur.fetchone()
    if row:
        clean_p, colored_p = row[0], row[1]
    else:
        clean_p, colored_p = None, None
    # —É–¥–∞–ª—è–µ–º –∑–∞–ø–∏—Å—å
    cur.execute("DELETE FROM merged_files WHERE id=?", (record_id,))
    conn.commit()
    conn.close()
    # —É–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if delete_files:
        for p in (clean_p, colored_p):
            try:
                if p and os.path.exists(p):
                    os.remove(p)
            except Exception:
                pass

def clear_history_db(delete_files: bool = False):
    """–û—á–∏—Å—Ç–∫–∞ —Ç–∞–±–ª–∏—Ü—ã; –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —É–¥–∞–ª–µ–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ —Å –¥–∏—Å–∫–∞."""
    if delete_files:
        recs = get_all_records_db()
        for r in recs:
            for p in (r.get("clean_path"), r.get("colored_path")):
                try:
                    if p and os.path.exists(p):
                        os.remove(p)
                except Exception:
                    pass
    conn = sqlite3.connect(str(DB_PATH))
    cur = conn.cursor()
    cur.execute("DELETE FROM merged_files")
    conn.commit()
    conn.close()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ë–î
init_db()

# ---------------------- Helpers (columns/types/filters/etc) ----------------------
def normalize_colname(s: str) -> str:
    s = (s or "").strip().lower()
    return re.sub(r"[^\w]+", "", s, flags=re.UNICODE)

def guess_id_column(df: pd.DataFrame) -> str:
    priors_exact = {"id", "–∏–¥"}
    priors_common = {
        "userid", "user_id", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª", "–Ω–æ–º–µ—Ä", "–∫–æ–¥",
        "clientid", "customerid", "–∫–æ–Ω—Ç—Ä–∞–≥–µ–Ω—Ç", "—Ç–∞–±–µ–ª—å–Ω—ã–π", "employeeid"
    }
    cols = list(df.columns)
    if not cols:
        return "id"
    norm_map = {c: normalize_colname(c) for c in cols}
    scores = {c: 0.0 for c in cols}
    for c, n in norm_map.items():
        if n in priors_exact:
            scores[c] += 5
        if n.endswith("id") or n == "id":
            scores[c] += 3
        if n in priors_common:
            scores[c] += 2
    for c in cols:
        s = df[c]
        non_null = s.notna().sum()
        if non_null == 0:
            continue
        uniq = s.dropna().astype(str).str.strip().nunique()
        uniq_ratio = uniq / max(1, non_null)
        non_null_ratio = non_null / max(1, len(df))
        scores[c] += uniq_ratio * 2 + non_null_ratio * 1.5
    best = max(scores, key=scores.get)
    return best

def to_str_id(series: pd.Series) -> pd.Series:
    return series.astype(str).str.strip()

def style_unmatched(df: pd.DataFrame):
    def row_style(row):
        return ['background-color: #ffe5e5'] * len(row) if row.get('__unmatched', False) else [''] * len(row)
    return df.style.apply(row_style, axis=1)

def infer_dtype(s: pd.Series) -> str:
    if pd.api.types.is_datetime64_any_dtype(s):
        return "datetime"
    if pd.api.types.is_numeric_dtype(s):
        return "number"
    if pd.api.types.is_bool_dtype(s):
        return "bool"
    nun = s.dropna().nunique()
    if nun > 0 and nun <= 50:
        return "category"
    return "text"

def apply_filters(df: pd.DataFrame, filters: dict) -> pd.DataFrame:
    res = df.copy()
    for col, f in (filters or {}).items():
        if col not in res.columns:
            continue
        t = f.get("type")
        if t == "number":
            vmin, vmax = f.get("range", (None, None))
            if vmin is not None:
                res = res[res[col] >= vmin]
            if vmax is not None:
                res = res[res[col] <= vmax]
        elif t == "datetime":
            start, end = f.get("range", (None, None))
            s = pd.to_datetime(res[col], errors="coerce")
            if start is not None:
                res = res[s >= pd.to_datetime(start)]
            if end is not None:
                res = res[s <= pd.to_datetime(end)]
        elif t == "category":
            vals = f.get("values")
            if vals:
                res = res[res[col].isin(vals)]
        elif t == "bool":
            val = f.get("value", None)
            if val is not None:
                res = res[res[col] == val]
        elif t == "text":
            substr = f.get("contains", "").strip()
            if substr:
                res = res[res[col].astype(str).str.contains(substr, case=False, na=False)]
    return res

# ---------- Utilities for saving files ----------
def unique_path_for(path: Path, allow_overwrite: bool = False) -> Path:
    if allow_overwrite or not path.exists():
        return path
    base = path.stem
    suffix = path.suffix
    for i in range(1, 1000):
        candidate = path.with_name(f"{base}_{i}{suffix}")
        if not candidate.exists():
            return candidate
    return path

def save_merged_files_to_disk(basename: str, clean_df: pd.DataFrame, styled_obj, merged_df: pd.DataFrame,
                              allow_overwrite: bool = False):
    base = re.sub(r"[\\/*?:\"<>|]+", "_", basename).strip()
    if not base:
        base = f"merged_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    clean_path = MERGED_DIR / f"{base}.xlsx"
    colored_path = MERGED_DIR / f"{base}_colored.xlsx"
    clean_path = unique_path_for(clean_path, allow_overwrite=allow_overwrite)
    colored_path = unique_path_for(colored_path, allow_overwrite=allow_overwrite)

    # save clean
    try:
        clean_df.to_excel(clean_path, index=False, engine="openpyxl")
    except Exception as e:
        raise RuntimeError(f"Could not save clean Excel: {e}")

    # try styled -> colored, fallback to merged_df
    try:
        if styled_obj is not None:
            styled_obj.to_excel(colored_path, engine="openpyxl", index=False)
        else:
            merged_df.to_excel(colored_path, index=False, engine="openpyxl")
    except Exception:
        try:
            merged_df.to_excel(colored_path, index=False, engine="openpyxl")
        except Exception as e:
            raise RuntimeError(f"Could not save colored Excel: {e}")

    return {
        "basename": base,
        "clean_path": str(clean_path),
        "colored_path": str(colored_path),
        "rows": int(len(clean_df)),
        "cols": int(len(clean_df.columns)),
        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M")
    }

# ---------------------- UI ----------------------
st.set_page_config(page_title="Excel Merger", layout="wide")

# CSS
st.markdown("""
<style>
.stApp { background: #f9f9fb; }
h1 { text-align:center; color:white !important;
    background: linear-gradient(90deg,#4CAF50,#2E7D32);
    padding:12px;border-radius:10px;font-size:26px;}
[data-testid="stFileUploader"] { border: 2px dashed #4CAF50; padding: 15px; border-radius:10px; background:#f9fff9; }
.app-content-padding-bottom { padding-bottom: 90px; }
</style>
""", unsafe_allow_html=True)

# Language / theme
lang = st.sidebar.selectbox("üåê Language / –Ø–∑—ã–∫ / Til / Ïñ∏Ïñ¥", options=["en","ru","uz","ko"],
                            index=1,
                            format_func=lambda x: {"en":"English","ru":"–†—É—Å—Å–∫–∏–π","uz":"O‚Äòzbek","ko":"ÌïúÍµ≠Ïñ¥"}[x])
t = translations[lang]
st.title(t["title"])

theme = st.sidebar.radio(t["theme"], [t["light"], t["dark"]], index=0)
if theme == t["dark"]:
    st.markdown("<style>.stApp{background:#121212;color:white;}.stDataFrame{color:white;}</style>", unsafe_allow_html=True)

# fixed footer overlay so it's visible even if st.stop is called earlier
def render_fixed_footer():
    color = "#666" if theme != t["dark"] else "#bbb"
    st.markdown(f"""
    <div style="position:fixed; left:0; bottom:0; width:100%; text-align:center; padding:6px 0; color:{color};
                background: rgba(0,0,0,0); font-size:13px; z-index:9999;">
      {t['footer']}
    </div>
    """, unsafe_allow_html=True)
    st.markdown("<div class='app-content-padding-bottom'></div>", unsafe_allow_html=True)

render_fixed_footer()

# File uploader
uploaded = st.file_uploader(t["upload"], type=["xlsx"], accept_multiple_files=True)

# Sidebar: history from DB
with st.sidebar.expander(t["history"], expanded=True):
    recs = get_all_records_db()
    if recs:
        for rec in recs:
            st.markdown(f"**{rec['basename']}** ‚Äî {rec['rows']}√ó{rec['cols']} ({rec['created_at']})")
            # download buttons
            cp = Path(rec["clean_path"])
            colp = Path(rec["colored_path"]) if rec.get("colored_path") else None
            if cp.exists():
                try:
                    with cp.open("rb") as fh:
                        b = fh.read()
                    st.download_button(f"‚¨áÔ∏è {cp.name}", data=b, file_name=cp.name,
                                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                       key=f"hist_dl_clean_{rec['id']}")
                except Exception as e:
                    st.warning(f"Could not prepare download for {cp.name}: {e}")
            else:
                st.caption("Clean file missing.")
            if colp and Path(colp).exists():
                try:
                    with Path(colp).open("rb") as fh:
                        b2 = fh.read()
                    st.download_button(f"‚¨áÔ∏è {Path(colp).name}", data=b2, file_name=Path(colp).name,
                                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                                       key=f"hist_dl_col_{rec['id']}")
                except Exception as e:
                    st.warning(f"Could not prepare download for {colp.name}: {e}")
            else:
                st.caption("Colored file missing.")
            # delete record
            if st.button(f"üóëÔ∏è Delete {rec['basename']}", key=f"del_db_{rec['id']}"):
                delete_record_db(rec['id'], delete_files=True)
                st.success("Record deleted.")
                st.experimental_rerun()
    else:
        st.caption("-")
    if st.button(t.get("clear_history", "Clear history")):
        clear_history_db(delete_files=False)
        st.success("History cleared.")
        st.experimental_rerun()

if not (uploaded and len(uploaded) >= 2):
    st.info(t["info_upload"])
    st.stop()

# Read uploaded files (do not add to DB)
raw_dfs, file_names = [], []
for f in uploaded:
    try:
        df = pd.read_excel(f)
        raw_dfs.append(df)
        file_names.append(f.name)
    except Exception as e:
        st.error(t["error_read"].format(name=f.name, error=e))
        st.stop()

# join type
join_type = st.sidebar.selectbox(t["join_type"], ["outer","inner","left","right"], index=0)

# Form: basename + ID/columns/filters
st.subheader(t["id_select"])
default_basename = f"final_merged_{datetime.now().strftime('%Y%m%d_%H%M')}"
merge_basename = st.text_input("üìÅ Final merged filename (basename, no extension)", value=default_basename)
merge_basename = re.sub(r"[\\/*?:\"<>|]+", "_", (merge_basename or default_basename)).strip()
if not merge_basename:
    merge_basename = default_basename

auto_save = st.checkbox(t.get("auto_save", "üîÅ Auto-save merged to server"), value=False)
allow_overwrite = st.checkbox("Overwrite existing merged files with same name", value=False)

id_cols = []
include_cols_per_file = []
filters_per_file = []

with st.form("id_select_form"):
    for i, (df, name) in enumerate(zip(raw_dfs, file_names), start=1):
        cols = list(df.columns)
        default_id = guess_id_column(df)
        col_id = st.selectbox(f"File {i}: {name}", options=cols,
                              index=cols.index(default_id) if default_id in cols else 0,
                              help=t["id_help"], key=f"id_{i}")
        id_cols.append(col_id)

        include_cols = st.multiselect(f"{t['include_cols']} ‚Äî {name}", options=cols, default=cols, key=f"inc_{i}")
        if col_id not in include_cols:
            include_cols = [col_id] + include_cols
        include_cols_per_file.append(include_cols)

        with st.expander(f"{t['filters']}: {name}", expanded=False):
            local_filters = {}
            for c in include_cols:
                s = df[c]
                dtype = infer_dtype(s)
                if dtype == "number":
                    s_num = pd.to_numeric(s, errors="coerce")
                    if s_num.notna().any():
                        try:
                            vmin = float(np.nanmin(s_num))
                            vmax = float(np.nanmax(s_num))
                            rng = st.slider(f"{name} | {c}", min_value=float(vmin), max_value=float(vmax),
                                            value=(float(vmin), float(vmax)), key=f"f_num_{i}_{c}")
                            local_filters[c] = {"type":"number", "range": rng}
                        except Exception:
                            pass
                elif dtype == "datetime":
                    s_dt = pd.to_datetime(s, errors="coerce")
                    if s_dt.notna().any():
                        dmin = s_dt.min().date()
                        dmax = s_dt.max().date()
                        rng = st.date_input(f"{name} | {c}", (dmin,dmax), key=f"f_dt_{i}_{c}")
                        if isinstance(rng, tuple) and len(rng)==2:
                            local_filters[c] = {"type":"datetime","range": rng}
                elif dtype == "bool":
                    val = st.selectbox(f"{name} | {c}", options=["‚Äî", True, False], index=0, key=f"f_bool_{i}_{c}")
                    if val != "‚Äî":
                        local_filters[c] = {"type":"bool","value": val}
                elif dtype == "category":
                    opts = sorted([("‚ÄîNaN‚Äî" if pd.isna(x) else str(x)) for x in s.unique()])
                    sel = st.multiselect(f"{name} | {c}", options=opts, default=[], key=f"f_cat_{i}_{c}")
                    def back(x): return np.nan if x=="‚ÄîNaN‚Äî" else x
                    if sel:
                        local_filters[c] = {"type":"category","values": list(map(back, sel))}
                else:
                    txt = st.text_input(f"{name} | {c}", value="", key=f"f_txt_{i}_{c}")
                    if txt.strip():
                        local_filters[c] = {"type":"text", "contains": txt.strip()}
            filters_per_file.append(local_filters)

    add_prefix = st.checkbox(t["prefix"], value=False)
    fill_value = st.text_input(t["nan_fill"], value="-", key="nanfill")
    submitted = st.form_submit_button(t["merge_button"])

if not submitted:
    st.info(t["info_wait"])
    st.stop()

# Apply selected columns and filters
prepared_dfs = []
id_sets = []
download_buffers = []

for df, name, idc, include_cols, fdict in zip(raw_dfs, file_names, id_cols, include_cols_per_file, filters_per_file):
    work = df.copy()
    cols_to_keep = [c for c in include_cols if c in work.columns]
    work = work[cols_to_keep].copy()
    work_filtered = apply_filters(work, fdict)

    if idc not in work_filtered.columns:
        st.error(t["error_no_id"].format(name=name))
        st.stop()

    dup_count = work_filtered[idc].duplicated(keep="first").sum()
    if dup_count:
        st.warning(t["warn_duplicates"].format(name=name, count=int(dup_count)))
        work_filtered = work_filtered.drop_duplicates(subset=[idc], keep="first").copy()

    work_filtered["id"] = to_str_id(work_filtered[idc])

    if add_prefix:
        other_cols = [c for c in work_filtered.columns if c not in [idc, "id"]]
        work_filtered = work_filtered.rename(columns={c: f"{name}__{c}" for c in other_cols})

    if idc != "id":
        work_filtered = work_filtered.drop(columns=[idc], errors="ignore")

    work_filtered = work_filtered.fillna(fill_value)

    prepared_dfs.append(work_filtered)
    id_sets.append(set(work_filtered["id"].tolist()))

    buf = BytesIO()
    work_filtered.to_excel(buf, index=False)
    buf.seek(0)
    download_buffers.append((name, buf))

# verify id presence
for i, dfp in enumerate(prepared_dfs, start=1):
    if "id" not in dfp.columns:
        st.error(t["error_no_id"].format(name=file_names[i-1]))
        st.stop()

# Merge
try:
    merged = reduce(lambda l,r: pd.merge(l, r, on="id", how=join_type), prepared_dfs)
except Exception as e:
    st.error(t["error_merge"].format(error=e))
    st.stop()

# presence, unmatched and sorting
presence_cols = []
for idx, (name, ids) in enumerate(zip(file_names, id_sets), start=1):
    colname = f"__present_in_{idx}"
    merged[colname] = merged["id"].isin(ids)
    presence_cols.append(colname)

merged["__present_count"] = merged[presence_cols].sum(axis=1)
merged["__unmatched"] = merged["__present_count"] < len(prepared_dfs)
merged_sorted = merged.sort_values(by=["__unmatched","id"]).reset_index(drop=True)

# Analytics
st.subheader(t["metrics"])
c1,c2,c3 = st.columns(3)
total_rows = len(merged_sorted)
fully_matched = int((merged_sorted["__present_count"] == len(prepared_dfs)).sum())
unmatched = int(merged_sorted["__unmatched"].sum())
c1.metric(t["m_total"], f"{total_rows:,}")
c2.metric(t["m_matched"], f"{fully_matched:,}")
c3.metric(t["m_unmatched"], f"{unmatched:,}")
dist = merged_sorted["__present_count"].value_counts().sort_index()
dist_df = pd.DataFrame({"files_present_in": dist.index.astype(int), "rows": dist.values})
st.caption(t["m_files_presence"])
st.bar_chart(dist_df.set_index("files_present_in"))

# Preview & interactive
st.subheader(t["preview"])
styled = style_unmatched(merged_sorted)
st.dataframe(styled, use_container_width=True)

st.subheader("üîç Interactive Table (filter, sort, hide columns)")
if AGGRID_AVAILABLE:
    gb = GridOptionsBuilder.from_dataframe(merged_sorted)
    gb.configure_pagination(enabled=True)
    gb.configure_default_column(editable=False, groupable=True, sortable=True, filter=True)
    gb.configure_side_bar()
    grid_options = gb.build()
    AgGrid(merged_sorted, gridOptions=grid_options, fit_columns_on_grid_load=True)
else:
    st.caption("AgGrid not available ‚Äî showing basic DataFrame")
    st.dataframe(merged_sorted)

tech_cols = ["__unmatched","__present_count"] + presence_cols
clean_df = merged_sorted[[c for c in merged_sorted.columns if c not in tech_cols]].copy()

# Prepare file names/paths and download/save buttons
clean_filename = f"{merge_basename}.xlsx"
colored_filename = f"{merge_basename}_colored.xlsx"
clean_path = MERGED_DIR / clean_filename
colored_path = MERGED_DIR / colored_filename

col1, col2 = st.columns(2)
with col1:
    out1 = BytesIO()
    clean_df.to_excel(out1, index=False, engine="openpyxl")
    out1.seek(0)
    st.download_button(t["download_clean"].format(name=merge_basename), data=out1,
                       file_name=clean_filename,
                       mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                       key=f"dl_clean_{merge_basename}")

    if st.button(t["save_merged"], key=f"save_{merge_basename}"):
        try:
            saved_meta = save_merged_files_to_disk(merge_basename, clean_df, styled, merged_sorted, allow_overwrite=allow_overwrite)
            # add to DB
            rec_id = add_record_db(saved_meta["basename"], saved_meta["clean_path"], saved_meta["colored_path"],
                                   saved_meta["rows"], saved_meta["cols"])
            st.success(f"Saved merged files as {saved_meta['basename']} and added to history (id={rec_id}).")
            st.experimental_rerun()
        except Exception as e:
            st.error(f"Could not save merged files: {e}")

with col2:
    out2 = BytesIO()
    try:
        if styled is not None:
            styled.to_excel(out2, engine="openpyxl", index=False)
        else:
            merged_sorted.to_excel(out2, index=False, engine="openpyxl")
        out2.seek(0)
        st.download_button(t["download_colored"].format(name=merge_basename), data=out2,
                           file_name=colored_filename,
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                           key=f"dl_colored_{merge_basename}")
    except Exception as e:
        try:
            out2 = BytesIO()
            merged_sorted.to_excel(out2, index=False, engine="openpyxl")
            out2.seek(0)
            st.download_button(t["download_colored"].format(name=merge_basename), data=out2,
                               file_name=colored_filename,
                               mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                               key=f"dl_colored_fallback_{merge_basename}")
            st.warning(t["error_styled"].format(error=e))
        except Exception as e2:
            st.error(t["error_styled"].format(error=e2))

# Auto-save
if auto_save:
    try:
        saved_meta = save_merged_files_to_disk(merge_basename, clean_df, styled, merged_sorted, allow_overwrite=allow_overwrite)
        # add to DB
        rec_id = add_record_db(saved_meta["basename"], saved_meta["clean_path"], saved_meta["colored_path"],
                               saved_meta["rows"], saved_meta["cols"])
        st.success(f"Auto-saved merged files as {saved_meta['basename']} and added to history (id={rec_id}).")
        st.experimental_rerun()
    except Exception as e:
        st.error(f"Auto-save failed: {e}")

# Download per-file filtered sources
with st.expander(t["download_filtered_each"], expanded=False):
    for name, buf in download_buffers:
        st.download_button(f"‚¨áÔ∏è {name}", data=buf, file_name=f"filtered_{name}.xlsx",
                           mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                           key=f"dlf_{name}")

with st.expander(t["expander"], expanded=False):
    st.markdown(t["expander_text"])

# Show DB-history table on main page
recs_main = get_all_records_db()
if recs_main:
    st.subheader(t["history"])
    # limited to last 50 for UI
    hist_slice = recs_main[:50]
    hist_df = pd.DataFrame(hist_slice)
    st.table(hist_df)

# bottom footer
st.markdown(f"<div style='text-align:center; padding:18px; color:#666;'>¬© {datetime.now().year}. {t['footer']}</div>", unsafe_allow_html=True)
